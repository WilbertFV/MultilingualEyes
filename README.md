Use this link to try it out yourself! 

https://multilingual-eyes-app.vercel.app/ 


#README for the Multilingual Eyes Multimodal Traveler's App

#Overview
Welcome to the Multilingual Eyes Multimodal Traveler's App repository! This innovative application represents a significant stride in enhancing navigational experiences using cutting-edge AI technologies. Designed with the needs of tourists and visually impaired individuals in mind, this app aims to provide an intuitive and inclusive way to interact with their surroundings.

#Prototype Description

The app is currently in its prototype phase, which serves as a proof-of-concept for the integration of multiple AI modalities, including vision and language processing. It embodies a user-centric design philosophy, focusing on practical reliability, inclusivity, and effectiveness for users with diverse abilities and language backgrounds.

#Core Features

Multimodal Interaction: The app integrates visual object detection using TensorFlow models and natural language processing capabilities.
Real-time Object Detection: Utilizing the device's camera, the app identifies and labels objects in the user's environment in real-time.
Multilingual Support: A key feature of this app is its ability to interact in multiple languages, including English, Spanish, Russian, French, and Chinese, making it accessible to a wider audience.
Voice and Text Interaction: Users can communicate with the app through both voice commands and text input, enhancing accessibility.
Responsive Design: The application is designed to be fully responsive, catering to various device types and screen sizes.

#Technologies Used

The Multilingual Eyes Multimodal Traveler’s App leverages a blend of advanced technologies to provide a seamless and inclusive navigational experience. Key technologies include:

TensorFlow.js and coco-ssd Model: Utilizing TensorFlow.js, a powerful and flexible machine learning library for JavaScript, the app incorporates the coco-ssd model. This model is essential for real-time object detection, enabling the app to recognize and label various objects within the camera's view.
React and Chat UI Kit: The app is built using React, a declarative, efficient, and flexible JavaScript library for building user interfaces. It integrates the Chat UI Kit for React, enhancing the chat interface's responsiveness and aesthetic appeal.
Web Speech API: This API is instrumental in the app's voice-to-text and text-to-voice functionalities, providing a hands-free experience and aiding users with visual impairments. It supports the app's accessibility features, allowing users to interact with the app using voice commands.
Multilingual Support: The app includes a language selection component, enabling users to choose from multiple languages, including English, Spanish, Russian, French, and Chinese. This multilingual support is crucial for enhancing the app's usability for a diverse user base.
OpenAI GPT Integration: Leveraging OpenAI's GPT multilingual capabilities, the app offers conversational AI interactions. This integration allows for dynamic and intelligent responses to user queries, enhancing the overall user experience.
Mobile Responsiveness: The app is designed with mobile responsiveness in mind, ensuring a consistent and user-friendly experience across various devices. This approach caters to the needs of travelers who predominantly use mobile devices for navigation.
Canvas API for Drawing: The Canvas API is used to draw bounding boxes and labels around detected objects, providing visual cues to users about the objects recognized by the app.
Custom Translation Functionality: The app features a custom translation module, which translates detected object labels into the user's selected language. This feature is essential for the app's multilingual capabilities, ensuring that users from different linguistic backgrounds can understand the information presented.

The combination of these technologies forms the backbone of the Multilingual Eyes Multimodal Traveler’s App, making it a versatile tool for navigation and exploration. The app stands out as a prototype that demonstrates the potential of integrating AI and machine learning in enhancing travel experiences, especially for tourists and individuals with visual impairments.

#Implications of Technology Integration

Based on our research and development, the integration of these technologies holds immense potential in various domains, particularly in enhancing accessibility and breaking language barriers in navigation and exploration contexts.

#Contribution and Community Access

We welcome contributions from the community to further develop and enhance the app's capabilities. The GitHub repository can be accessed at Multilingual Eyes, where you can find the source code, documentation, and contribution guidelines.

#Disclaimer

As this app is a prototype, it is important for users and contributors to understand that features and functionalities are subject to change, and the current version might not be fully optimized for all use cases.

Join us in shaping the future of multimodal, multilingual interaction and navigation technology!
